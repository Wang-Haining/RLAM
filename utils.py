import re
import torch
import numpy as np
from datasets import load_dataset, load_from_disk
from rouge_score import rouge_scorer
from datasets import load_metric
from sacrebleu.metrics import BLEU
from sacremoses import MosesTokenizer
from wordfreq import word_frequency
from nltk.tokenize import sent_tokenize
from transformers import AutoTokenizer

BASELINE_MODEL = "haining/sas_baseline"
DATASET_PATH = 'resources/scientific_abstract_simplification_corpus'
TOP_P = 0.9
SEED = 42
PROJECT_NAME = 'Scholarly_Abstract_Simplification'
MODEL_NAME = "google/gemma-2b"
RESPONSE_TEMP = "\n### Answer:"


# `is_punctuation` is adopted from
# github.com/cdimascio/py-readability-metrics/blob/master/readability/text/analyzer.py
def is_punctuation(token):
    match = re.match('^[.,\/#!$%\'\^&\*;:{}=\-_`~()]$', token)
    return match is not None


def compute_ari(text: str):
    """
    Compute the Automated Readability Index (ARI) for a given text.
    The ARI formula is: 4.71 * (characters/words) + 0.5 * (words/sentences) - 21.43
    Incomplete sentences (likely not ending in a period, exclamation, or question mark)
    are not considered.

    Args:
    text: A string of text to compute ARI.

    Returns:
        A list of tensors containing the processed rewards.
    """
    mt = MosesTokenizer(lang='en')
    sentences = sent_tokenize(text)
    words = mt.tokenize(text)
    # remove punctuation marks
    words = [w for w in words if not is_punctuation(w)]

    # check if the last sentence is complete
    if sentences and not sentences[-1].endswith((".", "?", "!")):
        # remove the last sentence if it is incomplete
        sentences = sentences[:-1]

    character_count = sum(len(word) for word in words)
    sentences_count = len(sentences)
    words_count = len(words)

    # avoid division by zero
    if sentences_count == 0 or words_count == 0:
        return 0

    # apply the ARI formula
    ari_score = (
            4.71 * (character_count / words_count)
            + 0.5 * (words_count / sentences_count)
            - 21.43
    )

    # clip for stability (assuming a reasonable ARI range)
    ari_score = max(min(ari_score, 35.0), 2.0)

    return ari_score


def is_jargon(word):
    # A lower frequency threshold means the word is less common and might be jargon
    threshold = 1e-6  # This threshold is arbitrary; adjust based on your needs
    frequency = word_frequency(word, 'en')
    return frequency < threshold


def build_dataset(
        model_name: str = "google/gemma-2b",
        task_prefix: str = ("### Simplify the scholarly abstract so it is immediately "
                            "understandable to a layperson: "),
        response_template: str = RESPONSE_TEMP
):
    """
    Build dataset for training. This function filters out too short samples and then
    extracts a specific number of samples for training.

    Args:
        model_name: SFT'ed model name.
        task_prefix: The prefix to prepend to each abstract for task
        instruction.
        response_template: RESPONSE_TEMP

    Returns:
        DataLoader: The DataLoader for the dataset.
    """
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    ds = load_from_disk(DATASET_PATH)
    ds = ds.rename_columns({"source": "query"})

    def tokenize(sample):
        # prepend the task-specific prefix
        input_text = task_prefix + sample["query"] + response_template
        input_ids = tokenizer.encode(
            input_text,
            truncation=True,
            max_length=1024,
        )
        sample["input_ids"] = torch.tensor(input_ids)
        sample["query"] = tokenizer.decode(sample["input_ids"],
                                           skip_special_tokens=True,
                                           clean_up_tokenization_spaces=True)

        return sample

    ds = ds.map(tokenize, batched=False)
    ds.set_format(type="torch")

    return ds


def collator(data):
    return dict((key, [d[key] for d in data]) for key in data[0])


def evaluate_outputs(predictions,
                     references,
                     sources,
                     all_metrics=False):
    """
    Evaluate model predictions against references.

    Args:
        predictions: List of strings generated by the model.
        references: List of reference strings.
        sources: List of source strings.
        all_metrics: If True, evaluate using all metrics (ARI, BLEU, SARI, ROUGE-L),
            otherwise only ARI and BLEU.

    Returns:
        A dictionary containing metric scores.
    """
    bleu_metric = BLEU()
    results = {}

    # compute ARI
    results['ari'] = [compute_ari(p) for p in predictions]

    # Compute BLEU scores
    results['bleu'] = [bleu_metric.corpus_bleu([p], [[r]]).score for p, r in
                       zip(predictions, references)]

    if all_metrics:
        # Compute SARI scores
        sari_metric = load_metric('sari')
        results['sari'] = np.mean([sari_metric.compute(predictions=[p],
                                                       references=[r],
                                                       sources=[s])['sari'] for p, r, s
                                   in zip(predictions, references, sources)])

        # Compute ROUGE-L scores
        rouge_metric = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        results['rougeL'] = [rouge_metric.score(p, r)['rougeL'].fmeasure for p, r in
                             zip(predictions, references)]

    return results

# mt = MosesTokenizer(lang='en')
# text = "This is an example sentence with CRISPR technology, and mRNA vaccines."
# tokens = mt.tokenize(text)
#
# # Filter and print only the tokens that are considered jargon and not punctuation
# jargon_tokens = [token for token in tokens if not is_punctuation(token) and
# is_jargon(token)]
# print(jargon_tokens)
