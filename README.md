# Reinforcement Learning from Accessibility Measures

This repository accompanies the paper *Science Out of Its Ivory Tower: Improving Accessibility With Reinforcement 
Learning*. Similar to RLHF (Reinforcement Learning from Human Feedback), we use reinforcement learning to enhance a 
language model's capabilities in specific ways.
But unlike RLHF, where a reward model is developed to model "human preference," we focus on document readability, 
which is much easier to quantify based on established measurements (e.g., Automatic Readability Index and 
Flesch-Kincaid Grade Level).
Instead of creating a reward model, we guided the optimization using heuristics such as average sentence length and word 
accessibility (defined as the natural logarithm of token occurrence per 1 billion tokens in English Wikipedia).
By carefully balancing word-level and sentence-level accessibility measures, our model achieved an additional 
improvement of three grade levels in readability without sacrificing faithfulness and language quality. 
The simplified abstracts generated by our model are roughly at the reading level of adults without a college degree.

## Training Logs

Training logs of the reported runs are hosted on [WANDB](https://wandb.ai/hainingwang/Reinforcement_Learning_from_Accessibility_Measures).

## Reproduction

To reproduce the results, follow these steps:

```bash
python3.10 -m venv .venv
source .venv/bin/activate
python -m pip install -r requirements.txt
```

### SFT and Word Accessibility Model

Refer to this [repository](https://github.com/Wang-Haining/scholarly_abstract_simplification) for details on the SFT 
and Word Accessibility Model.

### RLAM/RLARI

Refer to the 'runs' folder for training and evaluation scripts used to initiate runs on Slurm.

## Contact
hw56@indiana.edu

## License
0BSD

## Reference
TODO
